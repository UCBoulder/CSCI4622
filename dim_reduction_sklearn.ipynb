{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurricane meteorological data\n",
    "\n",
    "### Credit: Sophie Giffard-Roisin\n",
    "\n",
    "We will use a real hurricane meteorological dataset, which typical goal is to estimate the current stength of the hurricane or to predict its evolution.\n",
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/all_storms_since1979_IBTrRACKS_newcats.png?raw=true\" width=\"70%\">\n",
    "<div style=\"text-align: center\">Database: tropical/extra-tropical storm tracks since 1979. Dots = initial position, color = maximal storm strength according to the Saffir-Simpson scale.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "* numpy  \n",
    "* matplotlib\n",
    "* pandas \n",
    "* scikit-learn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Drop features `windspeed, stormid, instant_t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_p = Path(\"./data\")\n",
    "columns=['windspeed', 'stormid', 'instant_t']\n",
    "target = 'windspeed'\n",
    "df = ?\n",
    "y_tr = ?\n",
    "X_tr = ?\n",
    "X_tr.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the meaning of the columns, refer to this notebook https://github.com/ramp-kits/storm_forecast/blob/master/storm_forecast_starting_kit.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load also the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all features, transform your data such that mean=0 and std=1 (on the training data), and use the same parameters for transforming the test data also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component analysis\n",
    "\n",
    "Use [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and transform the training data such as to conserve 95% of the explained variance of the data. Then, transform the test data accordingly. (Look at sklearn.decomposition.PCA for how to use it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Fit the pca with the training features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(svd_solver='full')\n",
    "pca.fit(X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Calculate the cumulative explained variance (you can use the np.cumsum function) and determine how many modes are necessary in order to keep 95% of the explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum_var = np.cumsum(pca.explained_variance_)\n",
    "thresh = 0.95 * max(cumsum_var)\n",
    "for c, i in zip(cumsum_var, range(len(cumsum_var))):\n",
    "    if c > thresh:\n",
    "        num_modes = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(cumsum_var)\n",
    "plt.axhline(thresh, xmin=0, xmax=len(pca.explained_variance_))\n",
    "plt.show()\n",
    "print('Number of modes:' + str(num_modes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create a reduced feature matrix X_df_pca (and then X_df_test_pca) using the number of modes found. You may need to create a second 'pca' instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(svd_solver='full', n_components=num_modes)\n",
    "pca.fit(X_tr)\n",
    "X_tr_pca=pca.transform(X_tr)\n",
    "X_ts_pca=pca.transform(X_ts)\n",
    "print('Number of feature dimensions:'+ str(len(X_tr_pca[0])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the first two modes of the X_df_pca, with y as color label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.scatter(np.transpose(X_tr_pca)[0], np.transpose(X_tr_pca)[1], c=y_tr, s=1, cmap='jet')\n",
    "plt.colorbar()\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "plt.scatter(np.transpose(X_tr_pca)[0], np.transpose(X_tr_pca)[1], c=y_tr, s=1, cmap='jet')\n",
    "ax2.set_title('Without outliers')\n",
    "plt.xlim([-4,4])\n",
    "col = plt.colorbar()\n",
    "t = plt.suptitle('PCA modes 0 and 1, color = y (hurricane windspeed, knots):')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear methods: Multidimensional scaling (MDS) and Isomap\n",
    "\n",
    "The MDS performs a non-linear dimentionality reduction  by preserving the (Eucliean) distances between points. [The isomap](https://scikit-learn.org/stable/modules/manifold.html#isomap) is an extended version of the MDS where the geodesic distances are preserved. What is geodesic distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS, Isomap\n",
    "import warnings\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "warnings.simplefilter('ignore',SparseEfficiencyWarning)\n",
    "\n",
    "\n",
    "Nsamples = 1000\n",
    "X_tr_small = X_tr[:Nsamples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, apply the MDS to the training data X_df with 2 components and save it as X_df_MDS. Do the same thing with isomap and create a X_df_isomap. Verify that their shape are Nb_samples x 2 . Use less samples (1000) in order to reduce computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_MDS = MDS(n_components=2)\n",
    "X_tr_MDS = embedding_MDS.fit_transform(X_tr_small)\n",
    "\n",
    "embedding_isomap= Isomap(n_components=2)\n",
    "X_tr_isomap = embedding_isomap.fit_transform(X_tr_small)\n",
    "\n",
    "print(X_df_MDS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.subplot(1,2,1)\n",
    "ax.set_title('MDS with 2 components')\n",
    "plt.scatter(np.transpose(X_tr_MDS)[0], np.transpose(X_tr_MDS)[1],c=y_tr[:Nsamples], s=1, cmap='jet')\n",
    "c=plt.colorbar()\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.set_title('Isomap with 2 components')\n",
    "plt.scatter(np.transpose(X_tr_isomap)[0], np.transpose(X_tr_isomap)[1], c=y_tr[:Nsamples], s=1, cmap='jet')\n",
    "c2=plt.colorbar()\n",
    "\n",
    "\n",
    "t = plt.suptitle('MDS and Isomap, color = y (hurricane windspeed, knots):')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Isomap, we can maybe distinguish the trajectories of individual hurricanes (one hurricane has between 5 to 100 time steps, every time step is a different point here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARD : Regression with automatic dim. reduction\n",
    "\n",
    "Choose a smaller number of samples in order to reduce the computational time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ARDRegression\n",
    "Nsamples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit an ARD instance with part of the training samples (ex. 500 - you can shuffle them to have a better result):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ARDRegression(compute_score=True)\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the values of the feature weights and their histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Weights of the model\")\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.plot(clf.coef_, color='darkblue', linestyle='-', linewidth=2,\n",
    "         label=\"ARD estimate\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Values of the weights\")\n",
    "plt.legend(loc=1)\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "plt.title(\"Histogram of the weights\")\n",
    "plt.hist(clf.coef_, bins=len(X_tr[0]), color='navy', log=True)\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Values of the weights\")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see on the first figure what are the important features for this task. On the second, you can see that a lot of weights are 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, estimate the mean absolute error on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = clf.predict(X_tr_test)\n",
    "mae_ARD = mean_absolute_error(y_tr_test, y_est)\n",
    "print(mae_ARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods\n",
    "\n",
    "You can play with other dimensionality reduction on the same dataset (or on others) by looking at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim # for deciding whether to use it or not\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.manifold import SpectralEmbedding"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
