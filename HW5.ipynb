{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28eab32d65562b51fa31b15636074358",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 5 - Ridge, Lasso, Logistic Regressio, and Q-learning\n",
    "## CSCI 4622 - Fall 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Name**: $<$insert name here$>$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b8b72cbe8836c269315cd75a7a50569",
     "grade": false,
     "grade_id": "syllabus",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This assignment is due on Canvas by **11.59 PM on Friday, Nov 11th**.\n",
    "Submit only this Jupyter notebook to Canvas. Do not compress it using tar, rar, zip, etc.\n",
    "The last bonus problem will require additional files to be submitted. We'll explain its submission format later.\n",
    "\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question.\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors,\n",
    "but **you must write all code and solutions on your own**, and list any people or sources consulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt #uncomment to install all required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tests\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Ridge and Lasso (30pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "house_prices = data.HousePrices()\n",
    "plt.hist(house_prices.y_train, bins=40)\n",
    "plt.title(\"Scaled prices distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that Ridge regression adds a regularization term to the least square using the L2 norm.\n",
    "Ridge regression is part of scikit-learn package ([read more](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)), but we will be building our own implementation.\n",
    "You can test your implementation against scikit's.\n",
    "\n",
    "The objective of Ridge regression is to minimize:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{N}\\sum_{i=1}^{N} ||y_i-\\mathbf{w}^T \\mathbf{x_i}-\\mathbf{b}||^2 + \\alpha||\\mathbf{w}||^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{w}$ denotes the coefficients vector for the linear regression model,\n",
    "$\\mathbf{b}$ the intercept, $\\alpha$ the trade-off regularization parameter, and $N$ the number of samples.\n",
    "\n",
    "Luckily for us, Ridge regression admits a closed form solution for $\\mathbf w$ and $\\mathbf{b}$.\n",
    "\n",
    "Let $X$ be the $N\\times d$ matrix whose rows are the training samples $(\\mathbf{x_i})_{i\\leq N}$ and $Y=(y_i)_{i\\leq N}$ the target values.\n",
    "\n",
    "First, we start by centering the features (columns of X) by subtracting the mean of the column to get centered matrix $\\tilde{X}$. Then the solution for the minimization is (trust me):\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbf{w^*} = (\\tilde{X}^T\\tilde{X}+ \\alpha I)^{-1}\\tilde{X}^TY \\\\\n",
    "&\\mathbf{b^*} = \\frac{1}{N} \\sum_{i}^{i=N} (y_i - \\mathbf{w^*}^T \\mathbf{x_i})\n",
    "\\end{align}\n",
    "\n",
    "Note that:\n",
    "- The samples in $b^*$ are not centered.\n",
    "- The predicted targets would be $\\hat{y}_i = \\mathbf{w^*}^T \\mathbf{x_i}-\\mathbf{b^*} $\n",
    "- $\\tilde{X}$ is only needed in the `fit` method when we compute $\\mathbf{w^*}$ and $\\mathbf{b^*}$\n",
    "\n",
    "A common practice is to scale or normalize (usually normalize) the features before fitting the model. It allows a _fair_ treatment of different features.\n",
    "You are allowed to use scikit's `StandardScaler` to do the normalization (yes, a thing called scaler is normalizing. Don't @ me, @ sklearn). Do not forget to run the same transformation on the test data before running the prediction.\n",
    "\n",
    "**Q1.1[6 points]** Complete the `fit` and `evaluate` methods following the provided descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a0a657dc2b6ab46c3e4975f3c585fac",
     "grade": true,
     "grade_id": "cell-c3da2f9af6d23e34",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Ridge(object):\n",
    "\n",
    "    def __init__(self, alpha, normalize=False):\n",
    "        \"\"\"\n",
    "        :param alpha: regularization parameter\n",
    "        :param normalize: boolean whether to normalize the features or not\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = alpha  # our tuning / regularization parameter\n",
    "        self.coefficients = None  # our weights vector, w (in formulae above)\n",
    "        self.intercept = None  # our intercept parameter, b (in formulae above)\n",
    "        self.normalize = normalize  # boolean whether to normalize the features or not\n",
    "        self.scaler = StandardScaler()  # method by which to normalize the features (depends on self.normalize)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the ridge model, train it using the provided data\n",
    "        Calculate the number of non-zero coefficients in the model weights and the norm using np.linalg.norm\n",
    "        :param X: training features (num_samples, num_features)\n",
    "        :param y: target values (num_samples)\n",
    "        :return: tuple (number of non-zeros coefficients of w, norm of w)\n",
    "        \"\"\"\n",
    "        num_nonzero_coefs, coef_norm = 0, 0\n",
    "        # Workspace 1.1.a\n",
    "        # TO DO: compute w and b and store them in self.coef_ and self.intercept\n",
    "        # HINT: use self.scaler first, if and only if self.normalize is True\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return num_nonzero_coefs, coef_norm\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute Root mean square error (RMSE) between the predicted values and the actual values of the test data\n",
    "        :param X: instances array of shape (num_samples, num_features)\n",
    "        :param y: the true targets, of shape (num_samples)\n",
    "        :return: RMSE\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.1.b\n",
    "        # TO DO: predict based on the test features and return the root mean squared error\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tests cells, do not remove\n",
    "# Should run without errors\n",
    "tests.test_ridge_coef(Ridge, normalize=False)\n",
    "tests.test_ridge_coef(Ridge, normalize=True)\n",
    "tests.test_ridge_intercept(Ridge, normalize=True)\n",
    "tests.test_ridge_intercept(Ridge, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.2 **[6 points]** Produce 3 plots as a function of $\\log_{10}(\\alpha)$ (logarithmic scale) that compare normalized versus non-normalized Ridge:\n",
    "    - The number of non-zero coefficients of $\\mathbf w$\n",
    "    - The norm of $\\mathbf w$\n",
    "    - The test RMSE (Root Mean Squared Error)\n",
    "\n",
    "Use the values of $\\alpha$ provided in the cell. What is the best `alpha` for each version of the two models?\n",
    "\n",
    "To produce multiple plots in the same figure, see the examples [here](https://matplotlib.org/devdocs/gallery/subplots_axes_and_figures/subplots_demo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c615f55c912a6d63bdb3ee978002f9b4",
     "grade": true,
     "grade_id": "cell-97eea130546afcab",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.05, 0.1, 0.2, 0.3, 1.0, 10.0, 100.0, 300.0, 500.0, 1e3, 1.5e3, 2e3, 5e3, 1e4]\n",
    "# Workspace 1.2\n",
    "# BEGIN \n",
    "# code here\n",
    "# END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Based on the results from previous questions, you should have noticed that the interpretation of Ridge regression is not an easy task.\n",
    "One way to solve that is to use a regularization that adds _sparsity_ to $\\mathbf w$ and excludes less important features.\n",
    "That's what Lasso regression is about.\n",
    "\n",
    "Lasso uses $l_1$ norm in the regularization term and minimizes:\n",
    "\\begin{align}\n",
    "\\frac{1}{2N}\\sum_i ||y_i-\\mathbf{w}^t x_i -b||^2 + \\alpha||\\mathbf{w}||_1\n",
    "\\end{align}\n",
    "\n",
    "It is part of scikit package ([more details](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html))\n",
    "and you're free to compare your implementation to scikit's.\n",
    "\n",
    "- 1.3 **[6 points]** Complete the LassoRegression class by using scikit's Lasso in `self.model`. Save its parameters in `coefficients` and `intercept` (You're allowed to use inheritance for a more concise code, as long as the class has `coefficients` and `intercept` attributes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcb8bfd64deb523ecfa3eb975995ee29",
     "grade": true,
     "grade_id": "cell-4a5b01333c1af12f",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "class LassoRegression(object):\n",
    "    def __init__(self, alpha, normalize=False):\n",
    "        \"\"\"\n",
    "        :param alpha: regularization parameter\n",
    "        :param normalize: boolean whether to normalize the features or not\n",
    "        \"\"\"\n",
    "        self.model = Lasso(alpha=alpha)\n",
    "        self.coefficients = None  # our weights vector, w (in formulae above)\n",
    "        self.intercept = None  # our intercept parameter, b (in formulae above)\n",
    "        self.normalize = normalize  # boolean whether to normalize the features or not\n",
    "        self.scaler = StandardScaler()  # method by which to normalize the features (depends on self.normalize)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the lasso model, train it using the provided data\n",
    "        Calculate the number of non-zero coefficients in the model weights and the norm using np.linalg.norm\n",
    "        :param X: training features (num_samples, num_features)\n",
    "        :param y: target values (num_samples)\n",
    "        :return: tuple (number of non-zeros coefficients of w: scalar, norm of w: scalar)\n",
    "        \"\"\"\n",
    "\n",
    "        num_nonzero_coefs, coef_norm = 0, 0\n",
    "        # Workspace 1.3.a\n",
    "        # TO DO: compute w and b and store then in self.coef_ and self.intercept\n",
    "        # TO DO: call lasso_path on the centered features to compute self.coef_\n",
    "        # HINT: use self.scaler first, if and only if self.normalize is True\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return num_nonzero_coefs, coef_norm\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute Root mean square error (RMSE) between the predicted values and the actual values  of the test data\n",
    "        :param X: features array, shape (num_samples, num_features)\n",
    "        :param y: true targets, shape (num_samples)\n",
    "        :return: RMSE\n",
    "        \"\"\"\n",
    "        root_mean_squared_error = 0\n",
    "        # Workspace 1.3.b\n",
    "        # TO DO: predict based on the test features and return the mean_squared_error\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tests cell, do not remove\n",
    "# Test non-normalized Lasso\n",
    "tests.test_lasso_coef(LassoRegression, normalize=True)\n",
    "tests.test_lasso_coef(LassoRegression, normalize=False)\n",
    "tests.test_lasso_intercept(LassoRegression, normalize=True)\n",
    "tests.test_lasso_intercept(LassoRegression, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.4 **[6 points]** Produce 3 plots as a function of $\\log_{10}(\\alpha)$ (logarithmic scale) that compare normalized versus non-normalized Lasso:\n",
    "    - The number of non-zero coefficients of $\\mathbf w$\n",
    "    - The norm of $\\mathbf w$\n",
    "    - The test RMSE (Root Mean Squared Error)\n",
    "\n",
    "Use the values of $\\alpha$ provided in the cell. What is the best `alpha` for each version of the two models?\n",
    "\n",
    "Can you get all coefficients of $\\mathbf w$ to 0 for the non-normalized Lasso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a8fdd56444737248de79b15a23bbee1",
     "grade": true,
     "grade_id": "cell-a23427cbd9758986",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.05, 0.1, 0.2, 0.3, 1.0, 10.0, 100.0, 300.0, 500.0, 1e3, 1.5e3, 2e3, 5e3, 1e4]\n",
    "# Workspace 1.4\n",
    "# BEGIN \n",
    "# code here\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47d1afd6c13e01f357ce99a4e3897bf3",
     "grade": true,
     "grade_id": "cell-fef361cc010e5501",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 1.5 **[6 points]** (Write-up) Compare the two algorithms on the house prices dataset: compare the number of non-zero coordinates of Ridge vs Lasso and their RMSE on each dataset. Which type of regression is better? When does normalization improve the RMSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e22e0bf0f43cf7008c3f21cdf5673361",
     "grade": true,
     "grade_id": "cell-550acdf5a80b83f1",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Problem 2: Logistic Regression for Binary Classification (40 points + 8)\n",
    "\n",
    "The second part of this assignment will be dealing with Logistic Regression.\n",
    "While the name \"regression\" suggests otherwise, Logistic Regression is actually used for classification.\n",
    "It's a regression problem because the targets are the continuous likelihoods of the outcomes.\n",
    "\n",
    "Our dataset is the binary digits dataset. The class label is `0` if the digit is even and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "binary_digits = data.BinaryDigits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same notations from Problem 1 (this time with discrete targets $y_i$ in {0, 1}), _Logistic Regression_ is about minimizing the **N**egative **L**og **L**ikelihood objective defined as:\n",
    "\\begin{align}\n",
    "\\textrm{NLL}(\\mathbf{w}) = -\\frac{1}{N}\\sum_{i=1}^N \\left[y_i \\log \\sigma(\\mathbf{w}^T{\\mathbf{x_i}}) + (1-y_i)\\log(1 - \\sigma(\\mathbf{w}^T\\mathbf{x_i}))\\right]\n",
    "\\end{align}\n",
    "\n",
    "You might be wondering: where is the intercept?\n",
    "We're including the intercept in $(\\mathbf x_i)_i$ since we'll be adding a constant feature $\\mathbf x_{i,0} = 1$ to all samples $(\\mathbf x_i)_i$.\n",
    "We will call it the zero-th column and the intercept will be $\\mathbf{w}_0$.\n",
    "This zero-th column will be appended to the training samples in `fit` method and the test samples in `predict` method. $\\sigma$ is the sigmoid function seen in class.\n",
    "\n",
    "**Q2.1[5pts]** What is the derivative of the **NLL** w.r.t $\\mathbf{w}$ is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "297b43c084c10e1578f35fc6f9eb3914",
     "grade": true,
     "grade_id": "cell-0ebb96961f63756e",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2[5pts]** Complete the `sigmoid` function to return the sigmoid values $\\sigma(\\mathbf{w}^T{\\mathbf{x_i}})$ given features array. You have to truncate the score $\\mathbf{w}^T{\\mathbf{x_i}}$ to the interval $[-25, 25]$ to avoid overflow of `np.exp`\n",
    "\n",
    "**Q2.3[5pts]** Finish the `compute_gradient` function to return the derivative of the cost w.r.t. the weights\n",
    "\n",
    "**Q2.4[5pts]** Finish the `batch_update` function so that it performs batch gradient descent using the provided batch data\n",
    "\n",
    "**Q2.5[5pts]** Finish the `fit` function so that it performs several training epochs and returns the Recall score on the validation data at the end of each epoch. Initialize $\\mathbf{w}$ so that $\\mathbf{w}_j = \\frac{1}{d}$\n",
    "\n",
    "**Q2.6[2pts]** Finish the `predict` method to return the predicted labels (either 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08cc066ab1186bf46f6498ee325f7c6d",
     "grade": true,
     "grade_id": "cell-50279ea47b54c78a",
     "locked": false,
     "points": 22,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, eta=0.1, alpha=0):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param eta: Learning rate\n",
    "        :param alpha: We will use this parameter later (IN BONUS)\n",
    "        \"\"\"\n",
    "\n",
    "        self.w = None  # uninitialized w\n",
    "        self.eta = eta  # learning rate\n",
    "        self.initialized = False # flag used to initialize w only once, it allows calling fit multiple times\n",
    "        self.alpha = alpha  # regularization / penalty term (USED IN BONUS)\n",
    "\n",
    "    def sigmoid(self, x, threshold=25.0):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: features array of shape (num_samples, num_features + 1) (zero-th column appended)\n",
    "        :param threshold: the truncating threshold for np.exp, default to 25.0\n",
    "        :return: sigmoid values , of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        # Workspace 2.2\n",
    "        # TO DO: Complete this function to return the output of applying the sigmoid function to the score\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "\n",
    "    def compute_gradient(self, x, y):\n",
    "        \"\"\"\n",
    "        Return the derivative of the cost w.r.t to the weights. Don't forget to average by batch_size\n",
    "        :param x:  Feature vector, shape (batch_size, num_features +1), with zero-th column appended\n",
    "        :param y: real binary class label, shape (batch_size)\n",
    "        :return: gradient of shape (num_features + 1,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 2.3\n",
    "        # TO DO: Finish this function to compute the gradient\n",
    "        gradient = np.zeros((x.shape[1], ))\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return gradient\n",
    "\n",
    "    def batch_update(self, batch_x, batch_y):\n",
    "        \"\"\"\n",
    "        Single self.w update using the batch.\n",
    "        :param batch_x: array of features (includes the constant feature at column 0), of shape (batch_size, num_features + 1)\n",
    "        :param batch_y: array of target values, shape (batch_size,)\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 2.4\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "\n",
    "    def fit(self, X, y, epochs=1, batch_size=1, validation_X=None, validation_y=None):\n",
    "        \"\"\"\n",
    "        train the LogisticRegression\n",
    "        :param X: training features, shape (num_samples, num_features)\n",
    "        :param y: training labels, shape (num_samples,)\n",
    "        :param epochs: number of epochs, integer\n",
    "        :param batch_size: size of batch for gradient update, 1 for SGD\n",
    "        :param validation_X: validation rows, should default to training data if not provided\n",
    "        :param validation_y: validation labels\n",
    "        :return: recall score at the end of each epoch on validation data\n",
    "        \"\"\"\n",
    "\n",
    "        if validation_X is None:\n",
    "            validation_X, validation_y = X, y\n",
    "        metrics = []\n",
    "        # Workspace  2.5\n",
    "        # TO DO: Process X to append the zero-th constant column and call self.optimize\n",
    "        # TO DO: Compute average recall on the validation data at the end of each epoch\n",
    "        # HINT: make sure to initialize w\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return np.array(metrics)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: features array, shape (num_samples, num_features) (without the constant column)\n",
    "        :return: predicted binary label, shape (num_samples,)\n",
    "        \"\"\"\n",
    "        # Workspace 2.6\n",
    "        y_hat = np.zeros((X.shape[0],))\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)  # We append zero-th column\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return y_hat\n",
    "\n",
    "    def optimize(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Perform one epoch batch gradient on shuffled data\n",
    "        :param X: np.array of shape (num_samples, num_features +1), The training data with zero-th column appended\n",
    "        :param y: target values of shape (num_samples,)\n",
    "        :param batch_size: batch_size of the batch_update\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        indices = np.random.permutation(len(X))\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            batch_x = X[indices[i:i + batch_size]]\n",
    "            batch_y = y[indices[i:i + batch_size]]\n",
    "            self.batch_update(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the class above, loop over the training data and perform batch training with `batch_size=1`(Stochastic Gradient Descent) for 50 epochs, and different values of eta ( see `etas`). Train your model and do the following:\n",
    "\n",
    "**Q2.7[5pts]** Plot the recall trend for the different values of eta on the training data (epoch vs recall). (reset random seed for each loop to mitigate the randomness effect)\n",
    "\n",
    "Use the values provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10e27fda7275dff222839f326f147434",
     "grade": true,
     "grade_id": "cell-7e137aad3c9b1112",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "etas = [.001, .01, 0.1, 1]\n",
    "\n",
    "for eta in etas:\n",
    "    np.random.seed(42)  # Reset randomness\n",
    "    # Workspace 2.7\n",
    "    # BEGIN \n",
    "    # code here\n",
    "    # END\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.8[3pts]** Plot the recall trend for the different values of eta on the test data.\n",
    "(reset random seed for each loop to mitigate the randomness effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b791e8aa6629afd4f7cb39ed41d949af",
     "grade": true,
     "grade_id": "cell-540ca7c848249594",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "etas = [.0001, .001, .01, 1]\n",
    "\n",
    "for eta in etas:\n",
    "    np.random.seed(42)\n",
    "    # Workspace 2.8\n",
    "    # BEGIN \n",
    "    # code here\n",
    "    # END\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we want to analyze the effect of varying the batch size. We fix `eta=0.01` and `epochs=50` and we want to\n",
    "examine the recall on the test set at the end of the training.\n",
    "\n",
    "**Q2.9[5pts]** Produce a plot of the recall on the test data at the end of the training as a function of the batch size. Reset the random generator for each iteration.\n",
    "\n",
    "Use batch sizes provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4964d7b747c6de7c4712b78bbbae714",
     "grade": true,
     "grade_id": "cell-dd980cb8e349b0df",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_sizes = list(range(1, 40))\n",
    "recalls = []\n",
    "\n",
    "for b_size in batch_sizes:\n",
    "    np.random.seed(4622)\n",
    "    # Workspace 2.8\n",
    "    # BEGIN \n",
    "    # code here\n",
    "    # END\n",
    "plt.plot(batch_sizes, recalls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q2.10[Bonus 8 pts]** Since we're done with the binary regression, we will try to add Ridge regularization:\n",
    "\\begin{align}\n",
    "\\textrm{NLL}(w) = -\\frac{1}{N}\\sum_{i=1}^N \\left[y_i \\log \\sigma(\\mathbf{w}^T{x_i}) + (1-y_i)\\log(1 - \\sigma(\\mathbf{w}^Tx_i))\\right] + \\alpha {||\\mathbf{w}||^2}_{\\geq1}\n",
    "\\end{align}\n",
    "\n",
    "This is exactly what we will be using the `alpha` parameter in `LogisticRegression` for.\n",
    "First, write the gradient formula in the cell below and edit your `compute_gradient` to account for the regularization term. Note that the regularization $||w||^2$ does not apply to the intercept $\\mathbf{w}_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab3c9aff7aeb4220e393e2b0c0dfebc1",
     "grade": true,
     "grade_id": "cell-115ec1dd2a5dbf21",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 3: Q-Learning (30 points + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "596d6670442dd21f475e0b16f1313bc3",
     "grade": false,
     "grade_id": "rl_imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data\n",
    "import helpers\n",
    "import tqdm.notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "environment = data.GRID(grid_size=16, max_time=2000)\n",
    "environment.reset()\n",
    "environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "We'll be using a _4-rooms_ environment where the goal of the agent (white square) is to reach the target (red square).\n",
    "The grid is 16 by 16, so there are 256 states. Not all the states are accessible, since the agent can't cross the blue walls.\n",
    "Each state $s$ is represented as a tuple $(x_s, y_s)$ that reflects the position of the agent.\n",
    "\n",
    "There are 4 possible actions at each state (clockwise): Up (0), Right(1), Down(2), Left(3).\n",
    "The agent gets a penalty (reward = -1) if it hits the blue walls and a reward = 1 if it reaches the target.\n",
    "The episode ends (game over) when the target is reached or the agent has taken 2000 steps.\n",
    "\n",
    "We will be using Q-learning to find a good policy that will allow us to reach the target before the time runs out.\n",
    "\n",
    "At step $t$, the agent is located at state $s_t$ and chooses an action $a_t = \\pi(s_t)$ following the policy $\\pi$\n",
    "\n",
    "The agent then gets reward $r_t$ and moves to state $s_{t+1}$. The process repeats until we reach a final state $s_T$ (`game_over == True`)\n",
    "\n",
    "We'll call a replay a sequence of tuples $(s_t, a_t, r_t, s_{t+1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# How to create a replay with one element\n",
    "# repeat as long as game_over is false\n",
    "replay = []\n",
    "current_state = environment.reset()\n",
    "action = 2\n",
    "print(\"Initial state:{}\".format(current_state))\n",
    "new_state, reward, game_over = environment.step(action)  # down\n",
    "replay.append((current_state, action, reward, new_state))\n",
    "current_state = new_state\n",
    "print(\"Took action: {}\".format(data.grid.ACTIONS_NAMES[action]))\n",
    "print(\"New state: {}\\nReward: {}\\nGame over:{}\".format(new_state, reward, game_over))\n",
    "environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Starting at time $t=0$, The agent's goal is to maximize the expected discounted rewards:\n",
    "\\begin{align}\n",
    "\\eta_\\pi = \\sum_{t | a_t =\\pi(s_t)} \\gamma^t r_t\n",
    "\\end{align}\n",
    "$0<\\gamma<1$ is the discount rate. We'll be using $\\gamma = 0.95$\n",
    "\n",
    "$Q_\\pi(s_t, a_t)$ is defined as the expected discounted rewards if we start at state $s_t$, take an initial actions $a_t$, and\n",
    "follow the policy $\\pi$ to decide the remaining action $(a_{t+1},..... a_{T-1})$.\n",
    "\n",
    "\\begin{align}\n",
    "Q_\\pi(s,a) = \\eta_\\pi, \\text{ such that } s_0 = s \\text{ and } a_0 = a\n",
    "\\end{align}\n",
    "\n",
    "Since at each instant $t$, we want to pick the action that yields the best expected rewards, the optimal policy $\\pi^*$ would be:\n",
    "\\begin{align}\n",
    "\\pi^*(s) = \\arg\\max_a Q^*(s, a)\n",
    "\\end{align}\n",
    "\n",
    "__So how do we learn the optimal $Q^*$?__\n",
    "\n",
    "By exploring the environment and saving a replay $\\text{Replay} = \\{(s_t, a_t, r_t, s_{t+1})| t\\leq T\\}$ (one that reaches the goal, hopefully),\n",
    "\n",
    "we can use _Bellman equations_ to update our estimate of the $Q$ function in the following way:\n",
    "\\begin{align}\n",
    "Q_{new}(s_t, a_t) \\leftarrow r_t + \\gamma \\max_a Q_{old}(s_{t+1}, a), \\;\\;\\; \\forall \\; (s_t, a_t, r_t, s_{t+1})\\in \\text{Replay}\n",
    "\\end{align}\n",
    "\n",
    "We can prove that repeating this update will make our estimate converge to a unique optimal state-action value function $Q^*$.\n",
    "\n",
    "Instead of \"brutally\" updating Q, we'll use a learning rate $\\alpha$, so that the update would be :\n",
    "\\begin{align}\n",
    "Q_{new}(s_t, a_t) \\leftarrow  (1-\\alpha)Q_{old}(s_t, a_t) + \\alpha\\big[r_t + \\gamma \\max_a Q_{old}(s_{t+1}, a) \\big]\n",
    "\\end{align}\n",
    "\n",
    "In this first part, we'll use an array `Q` of shape (16 x 16 x 4) to store $Q$, and the value $Q(s,a)$ would be accessed via `Q[x_s, y_s, a]`.\n",
    "\n",
    "**Q3.1[6pts]** Complete the class method `TabularQ.update` to apply _Bellman_ updates provided a replay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee73741413d7a7e8c29b2af4ff5c1411",
     "grade": true,
     "grade_id": "tabularq",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TabularQ:\n",
    "    def __init__(self, gamma=0.95, learning_rate=0.01):\n",
    "        self.Q = np.zeros((16, 16, 4))\n",
    "        self.gamma = gamma\n",
    "        self.alpha = learning_rate\n",
    "\n",
    "    def update(self, replay):\n",
    "        for s_t, a_t, r_t, s_t_p_1 in replay:\n",
    "            # Workspace 3.1\n",
    "            # BEGIN \n",
    "            # code here\n",
    "            # END\n",
    "\n",
    "    def best_action(self, s):\n",
    "        action = np.argmax(self.Q[s[0], s[1]])\n",
    "        return action\n",
    "\n",
    "    def save(self, checkpoint_name):\n",
    "        np.savez_compressed(checkpoint_name, Q=self.Q)\n",
    "\n",
    "    def load(self, checkpoint_name):\n",
    "        self.Q = np.load(checkpoint_name +\".npz\" )[\"Q\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have to address a different question: when we explore the environment, how should we choose the actions?\n",
    "\n",
    "One approach is to follow a $\\epsilon$-greedy policy, where we choose a random action with probability $\\epsilon$ and the best action according to Q with probability $1-\\epsilon$.\n",
    "\n",
    "For instance, if $\\epsilon=1$, all actions are chosen randomly. For $\\epsilon=0$, all actions are chosen _greedily_.\n",
    "A common practice is to start with $\\epsilon=1$ and decay it to 0 as we generate replays. How fast we decay it is related to the famous _exploration-exploitation_ dilemma in Reinforcement Learning.\n",
    "\n",
    "**Q3.2[6pts]** Complete `EpsilonGreedy.act` to return a uniformly random action with probability $\\epsilon$ and the best action with probability $1-\\epsilon$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e7bd700e2b92b15adc61eaf4df809b4",
     "grade": true,
     "grade_id": "cell-3eae496de9e1c22a",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, Q, epsilon=1):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def act(self, state):\n",
    "        action = None\n",
    "        # Workspace 3.2\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.3[6pts]** Complete the cell below to generate a replay from one episode (until the game is over)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "081a979d428302f53a58d8675697a309",
     "grade": true,
     "grade_id": "cell-71f34c22b675aa94",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def play(environment, policy):\n",
    "    # Returns one episode's replay and the total accumulated rewards\n",
    "    replay = []\n",
    "    current_state = environment.reset()\n",
    "    game_over = False\n",
    "    total_rewards = 0\n",
    "    while not game_over:\n",
    "        # Workspace 3.3\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "    return replay, total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.4[6pts]** Complete the cell below to generate replays and update `tabular_Q` for `n_episodes`. You're free to choose your own decaying rate for $epsilon$ (including a 0 decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f856c833e3bde54d3f5aaa453228b137",
     "grade": true,
     "grade_id": "a42",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tabular_Q = TabularQ()\n",
    "n_episodes = 20\n",
    "greedy_policy = EpsilonGreedy(tabular_Q, epsilon=1.0)\n",
    "decay = 0.995\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "    # Workspace 3.4\n",
    "    # BEGIN \n",
    "    # code here\n",
    "    # END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's examine how the agent behaves a following random policy vs our Q-learning policy.\n",
    "\n",
    "You should notice that if you run Q-learning multiple times, you might get different policies (different ways to reach the target) : we've mentioned before that $Q^*$ is unique, it's not the case for $\\pi^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = play(environment, EpsilonGreedy(tabular_Q, epsilon=1.0)) # completely random policy\n",
    "helpers.save_frames(environment.episode, target_mp4=\"uniform\") # saves the episode to uniform.mp4\n",
    "helpers.display_video(\"uniform\") # display uniform.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Hint_ : if trained properly, you should expect \"Best policy reached target after\" < 30 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "episode_length = len(play(environment, EpsilonGreedy(tabular_Q, epsilon=0.0))[0]) # optimal policy\n",
    "helpers.save_frames(environment.episode, target_mp4=\"best_tabular\")\n",
    "helpers.display_video(\"best_tabular\") # best_tabular.mp4\n",
    "print(\"Best policy reached target after {} steps\".format(episode_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q3.5[6pts]** To get the full credit of the previous questions, you'll have to submit a zip file `q_learning.zip` that includes `uniform.mp4`, `best_tabular.mp4` from previous cells and `best_tabular.npz` from the cell below\n",
    "\n",
    "_Hint_:  If cells ran correctly, the files should appear in the same path as the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tabular_Q.save(\"best_tabular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Bonus: Deep Q Learning\n",
    "What made the tabular Q-learning more approachable is our knowledge of the state space. Our grid is 16 by 16, so there are 256 states in total (not all of them are accessible, due to the environment restrictions.)\n",
    "\n",
    "In the next Deep Q-learning section, we add randomness to our environment. In our _random_ grid, the target and the agents are randomly located, that yields $256^2 = 65536$ states. Not outside the realm of a tabular Q, but we will try to avoid that.\n",
    "\n",
    "When the grid is _random_, the state is no longer the (x,y) coordinates, but a 16x16 image of the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_environment = data.GRID(grid_size=16, random=True, max_time=200)\n",
    "s = random_environment.reset()\n",
    "print(s.shape, s.max(), s.min())\n",
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In Deep Q learning, we model Q as neural network $Q_\\theta$ where $\\theta$ represent the network parameters.\n",
    "The neural network takes the state image as input and outputs a 4-dimensional array $Q(s)$ where $Q(s)_i = Q(s, i) $ for $i\\in[0,1,2,3]$.\n",
    "\n",
    "Given a replay $Replay$, we will call an iteration:\n",
    "- Get the current $Q$ values of the states $s_t$ : `Q_s`\n",
    "- Get the current $Q$ values of the states $s_{t+1}$: `Q_s_p`\n",
    "- Updates the element of `Q_s` following __Bellman__ method (you'll have to use `Q_s_p`, the replay actions and the rewards)\n",
    "- Fit the neural network using the replays states and the computed `Q_s` for a single epoch\n",
    "\n",
    "A single update might consist of several iterations ( do not confuse this with the neural network fit's epoch)\n",
    "\n",
    "We'll be minimizing the objective\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_\\theta(\\text{Replay}) = \\sum_{(s_t, a_t, r_t, s_{t+1}) \\in \\text{Replay}} \\mathcal{L}\\big(Q_{\\theta}(s_t, a_t) - \\big[r_t + \\gamma \\max_a Q_\\theta(s_{t+1}, a) \\big] \\big)\n",
    "\\end{align}\n",
    "where $\\mathcal{L}$ is a loss function.\n",
    "\n",
    "In our implementation, we'll be using `Huber` loss. You're free to change the loss, the learning rate, and the network architecture. The ones provided bellow have been tested, and they're guaranteed to work\n",
    "\n",
    "**Q3.6[8pts]** Complete `DeepQ.update` to perform _Bellman_ iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f045d44c0f48a114bc13a3c6309c3f3",
     "grade": true,
     "grade_id": "a44",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "\n",
    "class DeepQ:\n",
    "    def __init__(self, gamma=0.95):\n",
    "        self.neural_net = models.Sequential([layers.InputLayer(input_shape=(16, 16, 3)),\n",
    "                                             data.get_inception_layer(32, 32, 32),\n",
    "                                             layers.Conv2D(64, 2, 2, activation=\"relu\"),\n",
    "                                             data.get_inception_layer(32, 32, 32),\n",
    "                                             layers.Conv2D(128, 2, 2, activation=\"relu\"),\n",
    "                                             layers.Conv2D(256, 2, 2, activation=\"relu\"),\n",
    "                                             layers.Flatten(),\n",
    "                                             layers.Dense(256, activation=\"tanh\"),\n",
    "                                             layers.Dense(512),\n",
    "                                             layers.Dense(4)])  # Do not change the model\n",
    "        optimizer = optimizers.adam_v2.Adam(learning_rate=1e-4)  # You can change the learning rate\n",
    "        self.neural_net.compile(loss=losses.Huber(), optimizer=optimizer)  # You can also change the loss function\n",
    "        self.neural_net.summary()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, replay, iterations=2, batch_size=32):\n",
    "        current_states = np.array([r[0] for r in replay])\n",
    "        actions = np.array([r[1] for r in replay])\n",
    "        rewards = np.array([r[2] for r in replay])\n",
    "        next_states = np.array([r[3] for r in replay])\n",
    "\n",
    "        loss = 0\n",
    "        for _ in range(iterations):\n",
    "            # Workspace 3.6\n",
    "            # Todo: predict Q_s and Q_s_p\n",
    "            # Use Q_s_p, actions, rewards to update Q_s and fit self.neural_network for one epoch\n",
    "            # Q_s = None\n",
    "            \n",
    "            # BEGIN \n",
    "            # code here\n",
    "            # END\n",
    "            history = self.neural_net.fit(current_states, Q_s, epochs=1, batch_size=batch_size)\n",
    "            loss += history.history[\"loss\"][0]\n",
    "        print(\"Loss:\", loss / iterations)\n",
    "\n",
    "    def best_action(self, s):\n",
    "        # We want to allow some noise in the estimation\n",
    "        q = self.neural_net.predict(s[None, :])[0]\n",
    "        q_max = np.max(q)\n",
    "        possible_actions = np.where(np.abs(q - q_max) < 0.05 * np.std(q))[0]\n",
    "        return np.random.choice(possible_actions)\n",
    "\n",
    "    def checkpoint(self, checkpoint_path):\n",
    "        self.neural_net.save_weights(checkpoint_path)\n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.neural_net.load_weights(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "deep_Q = DeepQ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to tweak the training parameters if you think it would improve learning. The provided ones have been tested and they should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "greedy_policy = EpsilonGreedy(deep_Q, epsilon=1.0)\n",
    "# Tweak the next parameters to improve the learning\n",
    "n_episodes = 80\n",
    "steps_per_replay = 8000\n",
    "decay= 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for e in range(n_episodes):\n",
    "    replay = []\n",
    "    episode = []\n",
    "    rewards = 0\n",
    "    pbar = tqdm.notebook.tqdm(desc=\"Generating replay\", total=steps_per_replay)\n",
    "    while len(replay) < steps_per_replay:\n",
    "        rep, rew = play(random_environment, greedy_policy)\n",
    "        episode += random_environment.episode\n",
    "        replay += rep\n",
    "        rewards += rew\n",
    "        pbar.update(len(rep))\n",
    "    pbar.close()\n",
    "    helpers.save_frames(episode, \"currently_training\") # save video of last episodes played\n",
    "\n",
    "    print(\"episode\", e + 1,\"/\",n_episodes,\", eps:\", greedy_policy.epsilon, \"total rewards:\", rewards)\n",
    "    deep_Q.update(replay, iterations=4)\n",
    "    greedy_policy.epsilon *= decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q3.7[2pts]** To be considered for full credit, include `deep_q.mp4` and the `checkpoint` folder in your submitted `q_learning.zip`\n",
    "\n",
    "_Hint_ : if trained properly, you should expect `\"Total rewards over 10 episodes` > 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "short_random_environment = data.GRID(random=True, max_time=100)\n",
    "episodes = []\n",
    "deep_best_policy = EpsilonGreedy(deep_Q, epsilon=0)\n",
    "rewards = 0\n",
    "for _ in tqdm.notebook.tqdm(range(10)):\n",
    "    rewards += play(short_random_environment, deep_best_policy)[1]\n",
    "    episodes += short_random_environment.episode\n",
    "print(\"Total rewards over 10 episodes:\", rewards)\n",
    "helpers.save_frames(episodes, \"deep_q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "helpers.display_video(\"deep_q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "deep_Q.checkpoint(\"checkpoint/deep_q\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
