{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homewor 4 - MLE, Gaussian Mixture, and SVM\n",
    "\n",
    "## CSCI 4622 - Fall 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Name**: $<$insert name here$>$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is due on Canvas by **11.59 PM on Friday, October 28th**.\n",
    "Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc.\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question.\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors,\n",
    "but **you must write all code and solutions on your own**, and list any people or sources consulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helpers\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 1 - MLE the Exponential Distribution Rate Parameter\n",
    "***\n",
    "\n",
    "Suppose you're given $m$ numbers $x_1, x_2, \\ldots, x_m$ (think training data) and told that they're samples from the exponential distribution $Exp(\\lambda)$ where the rate parameter $\\lambda$ is unknown. Recall that the probability density function for $Exp(\\lambda)$ is given by\n",
    "\n",
    "$$\n",
    "f_\\lambda(x) = \\left\\{\n",
    "\\begin{array}{rl}\n",
    "0 & \\textrm{if } x < 0 \\\\\n",
    "\\lambda e^{-\\lambda x} & \\textrm{if } x \\geq 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "In this problem we'll use Maximum Likelihood Estimation to estimate the rate parameter by hand and with a simulation\n",
    "\n",
    "**Q1.1 [5pts]** Write down the likelihood function $L(\\lambda)$ for the data set $x_1, x_2, \\ldots, x_m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a833e38129f7cb853aab5cbdcbaaa36",
     "grade": true,
     "grade_id": "cell-acee6e964068465c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2 [5pts]** Write down the associated Negative Log-Likelihood $\\textrm{NLL}(\\lambda)$ and simplify it algebraically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b787e74641dfe0d4758aec0a0b9bfb35",
     "grade": true,
     "grade_id": "cell-08e5d316380a5848",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3 [5pts]** Find a formula for the MLE of the rate parameter $\\lambda$ by taking the derivative of $\\textrm{NLL}(\\lambda)$, setting it equal to zero, and solving for $\\hat{\\lambda}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a88cba173e9cd426da2f0a97cddd2986",
     "grade": true,
     "grade_id": "cell-e80f064e92e1abec",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.4 [5pts]** Use the formula you found in **Q1.3** to estimate the rate parameter $\\lambda$ for `x_train` data for each `m`. Plot and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac07d364ff23fb9a2a50110416f067f7",
     "grade": true,
     "grade_id": "cell-cfd79555fbd1600f",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lam = 0.1\n",
    "Ms = np.arange(1, 100) * 100\n",
    "\n",
    "lam_hat = Ms * 0.0\n",
    "for i, m in enumerate(Ms):\n",
    "    x_train = np.random.exponential(1 / lam, size=m) + np.random.uniform(-.1, .1, m)\n",
    "    # Workspace 1.4\n",
    "    # BEGIN \n",
    "    # code here\n",
    "    # END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8f86913eee311afcf683e1d7c755b69",
     "grade": true,
     "grade_id": "cell-73d4fe42af57caee",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "For the remaining questions, we'll be using three synthetic datasets plotted below. From left to right: co-centric circles `circles`, blobs with 5 centers `multi_blobs`, and blobs with 2 centers `binary_blobs`.\n",
    "\n",
    "Each of the dataset instances has (`X`, `labels`) attributes that are split into `X_train, y_train` and `X_test, y_test` partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ac1bd6c35a12990e3fc389de623e63a",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Do not modify this cell\n",
    "circles = data.Circles()\n",
    "multi_blobs = data.DataBlobs(centers=4, std=1.5)\n",
    "binary_blobs = data.DataBlobs(centers=2, std=1.5)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_figheight(4), fig.set_figwidth(14)\n",
    "for i, (dataset, name) in enumerate([(multi_blobs, \"multi-blobs\"),\n",
    "                                     (circles, \"Co-centric circles\"),\n",
    "                                     (binary_blobs, \"binary blobs\")]):\n",
    "    axs[i].set_title(name)\n",
    "    axs[i].scatter(dataset.X[:, 0], dataset.X[:, 1], c=dataset.labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 : Gaussian Mixture Model (40 points)\n",
    "\n",
    "In a similar way to Problem 1, Gaussian Mixture Models (GMM) are based on the assumption that the data is generated from a certain distribution (mixture of $k$ Gaussian in this case), i.e:\n",
    "There exists Gaussians $\\{\\mathcal{N}(\\mu_1, \\Sigma_1)\\cdots \\mathcal{N}(\\mu_k, \\Sigma_k)\\}$ so that each sample $x_i$ is generated from one of the Gaussians. We work on $\\mathbb{R}^2$\n",
    "\n",
    "#### Notations:\n",
    "- $\\mu := (\\mu_1,...\\mu_k)$\n",
    "- $\\Sigma := (\\Sigma_1,... \\Sigma_k)$\n",
    "- $p_i$ the probability density function of $\\mathcal{N}(\\mu_i, \\Sigma_i)$\n",
    "- $\\pi=(\\pi_1,...\\pi_k)\\in(0,1)^k$ where $\\pi_i$ is the likelihood to generate samples from$\\mathcal{N}(\\mu_i, \\Sigma_i)$ (we can interpret $\\pi_i$ as the ratio of samples generated by the Gaussian $\\mathcal{N}(\\mu_i, \\Sigma_i)$).\n",
    "- $\\alpha_k(x) := \\frac{\\pi_k. p_i(x)}{\\sum_i \\pi_i. p_i(x)}$: the probabilty that data point $x$ was generated by the k-th Gaussian (You can check that $\\sum_i \\alpha_i(x) = 1$)\n",
    "\n",
    "Given data $\\mathcal{D}=\\{x_1,..., x_m\\}$ the expected log-likelihood is:\n",
    "\\begin{align}\n",
    "\\ell (\\pi,\\mu,\\Sigma) = \\sum_{i=1}^{m}\\sum_{j=1}^{k} \\alpha_j(x_i)\\left[\\log \\pi_j + \\log p_j(x_i)\\right]\n",
    "\\end{align}\n",
    "(we use tha natural log)\n",
    "\n",
    "**Q2.1[5pts]** Complete `sample_ll` to return the log likelihood of a single sample.\n",
    "\n",
    "**Q2.3[5pts]** Complete `assign` to return the index of the Gaussian that is most likely to have generated the sample.\n",
    "_Hint_: Use `GaussianMixture.alpha`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e4078400d81804c3636f840034371e0",
     "grade": true,
     "grade_id": "cell-ea14f504b19961f3",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class NormalDist:\n",
    "    \"\"\"Class Implementing a Gaussian\"\"\"\n",
    "    def __init__(self, mu=None, Sigma=None):\n",
    "        self.mu = mu or np.random.normal(0, 1, 2)\n",
    "        self.sig = Sigma or np.diag([0.5,0.5])\n",
    "\n",
    "    def pdf(self, x):\n",
    "        \"\"\" return the pdf of N(mu, Sigma) applied to x\n",
    "        Returns: array of shape (x.shape[0])\n",
    "        \"\"\"\n",
    "        return multivariate_normal(mean=self.mu, cov=self.sig).pdf(x)\n",
    "\n",
    "\n",
    "class GaussianMixture:\n",
    "    \"\"\"Class Implementing a Gaussian Mixture\"\"\"\n",
    "    def __init__(self, k):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k: Number of mixtures\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.pi = np.random.uniform(0, 1, k)\n",
    "        self.pi /= np.sum(self.pi)\n",
    "        self.gaussians = [NormalDist() for _ in range(k)]\n",
    "\n",
    "    def log_likelihood(self, X: np.ndarray):\n",
    "        return sum([self.sample_ll(d) for d in X])\n",
    "\n",
    "    def alpha(self, X):\n",
    "        # we add small value to avoid overflow\n",
    "        p = np.array([g.pdf(X) for g in self.gaussians]).T + 1e-8\n",
    "        alpha =  p * self.pi[None, :]\n",
    "        return  alpha / np.sum(alpha, axis=-1, keepdims=True)\n",
    "\n",
    "    def sample_ll(self, x):\n",
    "        # Workspace 2.1\n",
    "        log_like = 0.0\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return log_like\n",
    "\n",
    "    def assign(self, X):\n",
    "        clusters = None\n",
    "        # Workspace 2.2\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(4622)\n",
    "gmm = GaussianMixture(4)\n",
    "assert np.isclose(gmm.sample_ll(np.array([-.5,0.5])), -3.179732)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use Expectation-Maximization to learn $\\pi, (\\mu_1, \\Sigma_1),...(\\mu_k, \\Sigma_k)$. We assume that $\\Sigma_i = diag(\\sigma_{i, 1}, \\sigma_{i,2})$ is diagonal.\n",
    "\n",
    "Starting from randomly initialized parameters, we iterate the two steps:\n",
    "- E step: For each sample $x$, compute $\\alpha_k(x)$ (likelihood that $x$ is generated by the Gaussian k)\n",
    "- M step: Update $(\\pi, \\mu, \\Sigma)$ to maximize the likelihood:\n",
    "\\begin{align}\n",
    "\\sigma_{l,j}^2 &\\leftarrow \\frac{1}{\\sum_i \\alpha_l(x_i)} \\sum_i \\alpha_l(x_i) (x_{i,j} - \\mu_{l,j})^2 \\\\\n",
    "\\mu_k &\\leftarrow \\frac{1}{\\sum_i \\alpha_k(x_i)} \\sum_i \\alpha_k(x_i) x_i\\\\\n",
    "\\pi_k(x_i) & \\leftarrow \\frac{\\sum_i \\alpha_k(x_i) }{m}\n",
    "\\end{align}\n",
    "The iteration stops when the improvement ratio of the likelihood, `|new_likelihood - previous_likelihood/previous_likelihood |` is  less than `rtol`\n",
    "\n",
    "**Q2.3[15pts]** Complete `?_update` methods of GMM to implement the M step.\n",
    "\n",
    "**Q2.4[8pts]** Complete `fit` method by calling `?_update` methods and checking if the termination criterion is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a73f6a46554a28fe4294658172ec0217",
     "grade": true,
     "grade_id": "kmeans_code",
     "locked": false,
     "points": 23,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GMM(GaussianMixture):\n",
    "    def __init__(self, k, rtol=1e-4):\n",
    "        super().__init__(k)\n",
    "        self.plots = []\n",
    "        self.rtol = rtol\n",
    "        self.snapshots = []\n",
    "\n",
    "    def mu_update(self, alpha, X):\n",
    "        # Workspace 2.3.a\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "    def sig_update(self, alpha, X):\n",
    "\n",
    "        # Workspace 2.3.a\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "\n",
    "    def pi_update(self, alpha):\n",
    "        # Workspace 2.3.a\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Implement the GMM algorithm here as described above. Loop until the improvement ratio of the objective\n",
    "        is lower than rtol. At the end of each iteration, save the GMM objective and return the objective values\n",
    "        at the end\n",
    "\n",
    "        @param X:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        objective = self.log_likelihood(X)\n",
    "        history = [objective]\n",
    "        # Workspace 1.5\n",
    "\n",
    "        while True:\n",
    "            self.save_snapshot(X, self.predict(X))\n",
    "            alpha = self.alpha(X)\n",
    "            # BEGIN \n",
    "            # code here\n",
    "            # END\n",
    "            history.append(self.log_likelihood(X))\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.assign(X)\n",
    "\n",
    "    def save_snapshot(self, X, assignments):\n",
    "        \"\"\"\n",
    "        Saves plot image of the current assignments\n",
    "        \"\"\"\n",
    "        if X.shape[1] == 2:\n",
    "            self.snapshots.append(helpers.create_buffer(X, assignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show progress code\n",
    "%matplotlib notebook\n",
    "np.random.seed(4622)\n",
    "gmm = GMM(4)\n",
    "objective_history = gmm.fit(multi_blobs.X)\n",
    "helpers.show_progress(gmm.snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(objective_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q2.5 [7 points]** For each of the three datasets (`circles`, `multi_blobs`, and `binary_blobs`), do the following on the `training` partition of the data:\n",
    "- Fit a GMM with $k$ = # of unique labels.\n",
    "- Produce a scatter plot of the data points and color the points according to their *predicted labels*.\n",
    "\n",
    "Comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6368d5599e8ffa97474444db27b21b6",
     "grade": true,
     "grade_id": "a1_7",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_figheight(3), fig.set_figwidth(10)\n",
    "for i,(dataset, name, k) in enumerate([(circles, \"Cocentric circles\", 2),\n",
    "                                    (multi_blobs, \"multi-blobs\", 4),\n",
    "                                    (binary_blobs, \"binary blobs\", 2)]):\n",
    "    # Workspace 2.5\n",
    "    # BEGIN \n",
    "    # code here\n",
    "    # END\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5dc8f43e2a120a3a55e91275f49b569",
     "grade": true,
     "grade_id": "cell-8e399f0b5b4c3a77",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating GMM (5 Bonus pts)\n",
    "The easiest way to evaluate the clustering quality is to use the true labels. The natural question here is: which cluster corresponds to which label?\n",
    "\n",
    "Let's first formulate the question using `multi_blobs` dataset. We have 4 clusters and 4 classes in our data. Let's create a _confusion matrix_ $C$ between the clusters and the labels so that $ C_{i,j} = \\text{size}(\\text{cluster}_i \\cap \\text{class}_j)$.\n",
    "\n",
    "We model the unknown mapping using the $4\\times 4$ boolean matrix $X$ such that $X_{i,j}=1$ if and only if $\\text{cluster}_i$ is mapped to $\\text{class}_j$.\n",
    "\n",
    "To avoid having a cluster containing multiple classes, each row of $X$ is constrained to have only one non-zero entry.\n",
    "\n",
    "Now, given a mapping $X$ and confusion matrix $C$, the number of correctly \"classified\" points (not really classification, more of clustering here) is:\n",
    "\n",
    "\\begin{align}\n",
    "\\#\\text{correct} = \\sum_i \\sum_j C_{i,j} X_{i,j}\n",
    "\\end{align}\n",
    "The goal is to find $\\hat{X}$ that maximizes $\\#\\text{correct}$. To solve for $X$ we're going to use `scipy`'s `linear_sum_assignment`\n",
    "([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html)).\n",
    "\n",
    "- **Q2.6 [4 points]** Complete `evaluate_clustering` to return the accuracy of GMM(4) using the optimal mapping $\\hat{X}$ defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce37c23b00685ae7d2635c5473bce10b",
     "grade": true,
     "grade_id": "a26",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "def evaluate_clustering(trained_model, X, labels):\n",
    "    clusters = trained_model.predict(X)\n",
    "    # Workspace 2.6\n",
    "    # BEGIN \n",
    "    # code here\n",
    "    # END\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q2.7 [1 points]** Run GMM(4) on the full `multi_blobs` for 5 times. report the mean clustering performance and the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "310e3dd1e75ac6a63dda35ccf4ba65d0",
     "grade": true,
     "grade_id": "a27",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "accuracies = []\n",
    "# Workspace 2.7\n",
    "\n",
    "for _ in range(5):\n",
    "    gmm = GMM(4)\n",
    "    # BEGIN \n",
    "    # code here\n",
    "    # END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "403868c828b1e5edb289c2255b0f07f5",
     "grade": false,
     "grade_id": "SVM_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 3: Classification using Support Vector Machines and Kernel Trick (40 points)\n",
    "\n",
    "We have seen during the class the dual form of the Support Vector Machine problem using a kernel $K$:\n",
    "\n",
    "\\begin{aligned}\n",
    " \\max_{\\alpha} \\sum_i^m \\alpha_i &- \\frac{1}{2} \\sum_{i,j}^m y^{(i)}y^{(j)} \\alpha_i \\alpha_j K(x^{(i)},x^{(j)})\n",
    "    \\\\\n",
    "      s.t. \\text{   } \\alpha_i &\\geq 0 \\\\\n",
    "      \\sum_i^m \\alpha_i y^{(i)} &= 0\n",
    "\\end{aligned}\n",
    "\n",
    "The simplest kernel $K$ is the linear kernel \n",
    "\\begin{align}\n",
    "K_{lin}(x^{(i)},x^{(j)}) = <x^{(i)}, x^{(j)}>\n",
    "\\end{align}\n",
    " with $<.,.>$ being the scalar product.\n",
    "\n",
    " We'll be also using the radial kernel $K_{rad}$:\n",
    "\\begin{align}\n",
    " K_{rad, \\gamma}(x^{(i)},x^{(j)})  = \\exp \\big[-\\gamma ||x^{(i)} - x^{(j)}||^2]\n",
    "\\end{align}\n",
    "And the polynomial kernel $K_{poly, c, p}$:\n",
    "\\begin{align}\n",
    " K_{poly, c, p}(x^{(i)},x^{(j)})  = (<x^{(i)}, x^{(j)}> + c)^p\n",
    "\\end{align}\n",
    "\n",
    "**Q3.1. [12 pts]** Complete the implementation of the three kernels defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2956f38c5f5b3d19ef92b3661194769",
     "grade": true,
     "grade_id": "a2_1",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LinearKernel(object):\n",
    "    def compute(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute the kernel matrix\n",
    "        @param x1: array of shape (m1,p)\n",
    "        @param x2: array of shape(m2,p)\n",
    "        @return: K of shape (m1,m2) where K[i,j] = <x1[i], x2[j]>\n",
    "        \"\"\"\n",
    "        # Workspace 3.1.a\n",
    "        K = np.zeros((x1.shape[0], x2.shape[0]))\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return K\n",
    "\n",
    "\n",
    "class RadialKernel(object):\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute the kernel matrix. Hint: use sklearn's Euclidean distance \n",
    "        @param x1: array of shape (m1,p)\n",
    "        @param x2: array of shape(m2,p)\n",
    "        @return: K of shape (m1,m2) where K[i,j] = K_rad(x1[i],x2[j]) = exp(-gamma * ||x1[i] - x2[j]||^2)\n",
    "        \"\"\"\n",
    "        # Workspace 3.1.b\n",
    "        K = np.zeros((x1.shape[0], x2.shape[0]))\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return K\n",
    "\n",
    "\n",
    "class PolynomialKernel(object):\n",
    "\n",
    "    def __init__(self, c, p):\n",
    "        self.c = c\n",
    "        self.p = p\n",
    "\n",
    "    def compute(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute the kernel matrix.\n",
    "        @param x1: array of shape (m1,p)\n",
    "        @param x2: array of shape(m2,p)\n",
    "        @return: K of shape (m1,m2) where K[i,j] = (x1[i].x2[j] + c)^p\n",
    "        \"\"\"\n",
    "        # Workspace 3.1.c\n",
    "        K = np.zeros((x1.shape[0], x2.shape[0]))\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5309d156112a518679291369ab8abba",
     "grade": false,
     "grade_id": "cvxopt",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll solve for $\\alpha$ of the dual form  using the quadratic solver from [`cvxopt` package](https://cvxopt.org/userguide/coneprog.html#quadratic-programming).\n",
    "To match the solver API, we need to rewrite the problem as:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\min \\frac{1}{2} x^TPx + q^Tx\n",
    "    \\\\\n",
    "     s.t. \\ Gx \\leq h\n",
    "    \\\\\n",
    "    \\ Ax = b\n",
    "\\end{aligned}\n",
    "\n",
    "So we'll define $P, q, G, h, A, b$  as:\n",
    "\n",
    "\\begin{align}\n",
    "P_{i,j} &= y^{(i)}y^{(j)} K(x^{(i)},x^{(j)}) \\text{,  matrix $P$ is of shape $m\\times m$}\\\\\n",
    "q &= -\\overline{1} \\text{,  vector of size m} \\\\\n",
    "G &= diag(-\\overline{1}) = -I_m\\text{, diagonal matrix of -1} \\\\\n",
    "h &= \\overline{0} \\text{,  vector of size m} \\\\\n",
    "A &= y \\text{, the labels vector} \\\\\n",
    "b &= 0 \\text{, a scalar (not to confuse with the intercept of SVM)}\n",
    "\\end{align}\n",
    "\n",
    "- **3.2 [8 points]** Complete `quadratic_solver` by defining the different arrays $P, q, G, h, A$ and the scalar $b$. Make sure all arrays are of float type (it is important for `cvxopt`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f118cd7c1fbeeb92c8fd8dceb75d701",
     "grade": true,
     "grade_id": "a2_2",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "import numpy as np\n",
    "\n",
    "solvers.options['show_progress'] = False\n",
    "solvers.options['abstol'] = 1e-10\n",
    "solvers.options['reltol'] = 1e-10\n",
    "solvers.options['feastol'] = 1e-10\n",
    "\n",
    "\n",
    "def quadratic_solver(K, y, beta=0.0):\n",
    "    \"\"\"\n",
    "    Solve for alpha of the dual problem, \n",
    "    @param K: Kernel matrix K of shape (m,m)\n",
    "    @param y: labels array y of shape (m,)\n",
    "    @return: optimal alphas of shape (m,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Workspace 3.2\n",
    "    m = K.shape[0]\n",
    "    #P = ? # shape (m,m)\n",
    "    #q = ? # shape(m,1)\n",
    "    #G = ? # shape(m,m)\n",
    "    #h = ? # shape (m,)\n",
    "    #A = ? # shape (1,m)\n",
    "    #b = ? # scalar\n",
    "    # BEGIN \n",
    "    # code here\n",
    "    # END\n",
    "    h = np.zeros(m)  #shape (m,)\n",
    "    A = y.reshape(1, -1)  # shape (1,m)\n",
    "    b = 0.0  # scalar\n",
    "    sol = solvers.qp(P=matrix(P.astype(float)),\n",
    "                     q=matrix(q.astype(float)),\n",
    "                     G=matrix(G.astype(float)),\n",
    "                     h=matrix(h.astype(float)),\n",
    "                     A=matrix(A.astype(float)),\n",
    "                     b=matrix(b))\n",
    "    alphas = np.array(sol['x'])\n",
    "    alphas = alphas * (np.abs(alphas) > 1e-8)  # zeroing out the small values\n",
    "    return alphas.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4569ce1b4f599e8afbaa3177252d3282",
     "grade": false,
     "grade_id": "svm_desc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Once we get the optimal $\\alpha$, then we can get the indices of the support vectors $S = \\{i | \\alpha_i >0 \\}$. The intercept $b$ is computed as:\n",
    "\n",
    "\\begin{align}\n",
    "b = \\frac{1}{|S|}\\sum_{m\\in S}\\big[ y^{(m)} - \\sum_{i\\in S} \\alpha_i  y^{(i)}K(x^{(i)}, x^{(m)})\\big],\n",
    "\\end{align}\n",
    "(we only sum over the support vectors, i.e for which $\\alpha>0$)\n",
    "\n",
    "and the prediction for a point $x$ would be:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = \\text{sign}\\big[\\sum_i y^{(i)}\\alpha_i K(x,x^{(i)}) + b \\big],\n",
    "\\end{align}\n",
    "The sum is originally over the support vectors, since for non-support vectors $\\alpha=0$ we can sum over the entire training data.\n",
    "\n",
    "You can see that we only need access to the kernel $K$ and the model is oblivious to the features themselves. That's the power of kernel methods!\n",
    "\n",
    "- **3.3 [8 points]** Complete the `fit` method of SVM. You have to transform the 0-1 $y$ labels to (-1,1) before doing the computations since the SVM formulation assumes that the binary labels are (-1,1).\n",
    "\n",
    "- **3.4 [5 points]** Complete the `predict` method to return the predicted labels of the provided points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "997b0eb265005b5b802a6a896518f455",
     "grade": true,
     "grade_id": "svm_code",
     "locked": false,
     "points": 13,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel, beta=0.0):\n",
    "        self.kernel = kernel\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.intercept = None\n",
    "        self.alphas = None\n",
    "        self.beta = beta  # ridge regularization coefficient\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Transform y to (-1,1) and use self.kernel to compute K\n",
    "        Solve for alphas and compute the intercept using the provided expression\n",
    "        Keep track of X and y since you'll need them for the prediction\n",
    "        @param X: data points of shape (m,p)\n",
    "        @param y: (0,1) labels of shape (m,)\n",
    "        @return: None\n",
    "        \"\"\"\n",
    "        # Workspace 3.3\n",
    "        self.X = X\n",
    "        self.y = 2 * y - 1\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels of points in X\n",
    "        @param X: data points of shape (m,p)\n",
    "        @return: predicted 0-1 labels of shape (m,)\n",
    "        \"\"\"\n",
    "        # Workspace 3.4\n",
    "        predicted_labels = np.zeros((X.shape[0],))\n",
    "        # BEGIN \n",
    "        # code here\n",
    "        # END\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6ae3b7677901f88c737ed73c2cbf81d",
     "grade": false,
     "grade_id": "q2_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We provide below an example of the expected plots for this problem using the linear kernel. \n",
    "\n",
    "- **3.5 [2 points]** Edit the cell to report the accuracy on the test sets for each of the two datasets visualized in the plots. How do you explain the obtained accuracies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26ef4d33b4495c61b7fd5605e8efcc18",
     "grade": true,
     "grade_id": "a2_5_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN \n",
    "# code here\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b659ed5154e6d78cbf3c08569e44ec5",
     "grade": true,
     "grade_id": "a2_5_2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b5788592336ed3779668ac25ac90a3f",
     "grade": false,
     "grade_id": "q2_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- **3.6 [3 points]** Plot and report SVM performance on the same datasets as in 2.5 using the radial kernel with $\\gamma=2.0$. Describe the model performance and compare it to the linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79812f54ecb67c068ed0636258793473",
     "grade": true,
     "grade_id": "a2_6_1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN \n",
    "# code here\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8809ac8bf2e15743839d96c965196297",
     "grade": true,
     "grade_id": "a2_6_2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "995580061470254c4be1885dbb0888a5",
     "grade": false,
     "grade_id": "q2_7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- **3.7 [2 points]** Plot and report SVM performance on the same datasets as in 2.5 using the polynomial kernel with $(c,p) = (1,5)$. Describe the model performance and compare it to the two previous kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0e72f4e0bd2ab78f9856783040bed25",
     "grade": true,
     "grade_id": "a2_7_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN \n",
    "# code here\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b12601586bde421506ab8ca243f0e16d",
     "grade": true,
     "grade_id": "a2_7_2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "% BEGIN\n",
    "\n",
    "% YOUR ANSWER HERE\n",
    "\n",
    "% END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Regularization (4 Bonus pts)\n",
    "\n",
    "We add a ridge regularization of $\\alpha$ with coefficient $\\beta$, this time there is a minus $-$ since we're maximizing (so that $\\beta ||\\alpha||^2$ is minimized).\n",
    "\\begin{aligned}\n",
    " \\max_{\\alpha} \\Big[ \\sum_i^m \\alpha_i &- \\frac{1}{2} \\sum_{i,j}^m y^{(i)}y^{(j)} \\alpha_i \\alpha_j K(x^{(i)},x^{(j)}) - \\frac{\\beta}{2} ||\\alpha||^2 \\Big]\n",
    "    \\\\\n",
    "      s.t. \\text{   } \\alpha_i &\\geq 0 \\\\\n",
    "      \\sum_i^m \\alpha_i y^{(i)} &= 0\n",
    "\\end{aligned}\n",
    "\n",
    "**Q3.8 [Bonus, 4pts]** Which elements of the quadratic system $P, q, G, h, A, b$ do we have to Change? Edit **Q3.2** and `SVM` to incorporate `beta` argument."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
